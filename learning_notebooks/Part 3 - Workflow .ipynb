{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store sales predictions \n",
    "\n",
    "You are being asked to predict the sales of a bunch of stores, during the next 3 days.\n",
    "\n",
    "You have a train and test set, as in the hackathon, and don't know the labels in the test set. \n",
    "\n",
    "The success metric is you small your mean absolute error is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General advice: \n",
    "\n",
    "- Get to a base result fast and early \n",
    "\n",
    "\n",
    "- Keep your functions well organized \n",
    "\n",
    "\n",
    "- Remember to sample intelligently. You are unlikely to want to train on all data every time \n",
    "\n",
    "\n",
    "- When dealing with a multi-index with lots of timeseries, explore a few individual ones to get an idea of the data\n",
    "\n",
    "\n",
    "- Rolling train-test-predict may be tempting and elegant, but realistically you are unlikely to ever have enough CPU. \n",
    "\n",
    "\n",
    "- Use Index Slices to ensure legibility. You'll thank yourself later. \n",
    "\n",
    "\n",
    "- Don't go straight for the more sophisticated answers. Build up from the simper ones.\n",
    "\n",
    "\n",
    "- Remember the business scope. What are we being asked to optimize here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual imports \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import statsmodels.api as sm \n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  \n",
    "% matplotlib inline \n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super basic understanding of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows and columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making it a datetime multi index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly per-store and per-date, so let's make it a multi index. \n",
    "\n",
    "First, make the date a datetime: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Date = pd.to_datetime(train.Date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of dates? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.Date.min())\n",
    "print(train.Date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's set the index then: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.set_index(['Date', 'Store'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always, always, always sort your index, or face a world of pain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring: single store analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe a single store: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember index slicing? \n",
    "idx = pd.IndexSlice\n",
    "store_1 = train.loc[idx[:, 1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the store's sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_1.reset_index().set_index('Date').Sales.plot(figsize=(16, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our store seems to close on certain days. Maybe these are weekends? What's the mean sales per day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get level values is a good way of getting the date values from the index, but there are others. \n",
    "\n",
    "store_1.groupby(store_1.index.get_level_values('Date').weekday_name).Sales.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got it, so Sundays are no sales days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split \n",
    "\n",
    "Let's break off a few days to predict as a test set, and see if we can do it: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We get the biggest timestamp in our index (i.e. the \"maximum\" date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day_in_index = train.index.get_level_values('Date').max()\n",
    "print(last_day_in_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We want to predict only 4 days. This means that those days won't be in the training set.  To be the dates where the training set ends (`new_last_day_train`) and where the test starts (`new_first_day_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's peel off 4 days \n",
    "new_last_day_train = last_day_in_index - pd.DateOffset(days=4)\n",
    "new_first_day_test = last_day_in_index - pd.DateOffset(days=3)\n",
    "\n",
    "print('new_last_day_train', new_last_day_train)\n",
    "print('new_first_day_test', new_first_day_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Finally, we split all time series at once using the index slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new train and new test \n",
    "new_train = train.loc[idx[:new_last_day_train, :], :]\n",
    "new_test = train.loc[idx[new_first_day_test:, :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the test set is really only 4 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.index.get_level_values('Date').unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our train test split!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline early "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now for a stupid question: how well would predict if we just predicted the last day's sale of each store, for every one of the next 4 days? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day = new_train.loc[idx[new_last_day_train]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our dumbass prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day.Sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to predict stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 1 of test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['predictions'] = 0 \n",
    "new_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_test_set = new_test.index.get_level_values('Date').unique()\n",
    "days_in_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days_in_test_set: \n",
    "    new_test.loc[idx[day, :], 'predictions'] = last_day.Sales.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how well our approaches to modeling/prediction are, let's use [Mean Absolute Error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html). It is easy to interpret and we have the function implemented in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=new_test['Sales'], y_pred=new_test['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we were unlucky, and got a Sunday in our test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_test_set.weekday_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blast! A Sunday! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the Sunday predictions to Zero, and see where that puts us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.loc[new_test.index.get_level_values('Date').weekday_name=='Sunday', 'predictions'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=new_test['Sales'], y_pred=new_test['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. To get an idea, what is the order of manitude of our sales? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.Sales.hist(bins=20, figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the sales aren't 0, what's the median sale for one of our stores? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.Sales.loc[new_train.Sales > 0].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're having an error which is approximately 1400, on sales whic are approximately 6500.\n",
    "\n",
    "Now let's beat our stupid baseline, which is `1377`! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build on the previous baseline, with simple ideas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our second attempt, we'll do the following: take the previous sale, of each store, on the same weekday. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to see the Fridays in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridays = new_train.loc[new_train.index.get_level_values('Date').weekday_name == 'Friday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridays.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe stores are relatively consistent accross days of the week? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridays.loc[idx[:, [1, 2, 3]], 'Sales'].unstack().plot(figsize=(16, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. How do I get the most recent \"Friday\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent = fridays.index.get_level_values('Date').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, they seem to have had some big slumps along the way, but other than that, fairly stable. Let's try this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just reset the predictions from our little test set... \n",
    "new_test['predictions'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days_in_test_set:\n",
    "    day_of_week = day.weekday_name\n",
    "    same_day_of_the_week_data = new_train.loc[new_train.index.get_level_values('Date').weekday_name == day_of_week]\n",
    "    date_of_most_recent = same_day_of_the_week_data.index.get_level_values('Date').max()\n",
    "    \n",
    "    new_test.loc[idx[day, :], 'predictions'] = new_train.loc[idx[date_of_most_recent, :], 'Sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any wins? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=new_test['Sales'], y_pred=new_test['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, get your timeseries modelling face on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important caveat. The techniques we taught you with ARIMA and SARIMAX are interesting to know, but in reality common sense is your main tool. Having made that caveat, let us proceed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose a single store, to see what we can find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib image size \n",
    "plt.rcParams['figure.figsize'] = (16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a store \n",
    "a_store = new_train.loc[idx[:, 4], 'Sales']\n",
    "\n",
    "# remove the level of the index that just says \"4, 4, 4, 4 ...\" because it's always the same store\n",
    "\n",
    "a_store.index = a_store.index.droplevel('Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(a_store, model='additive')\n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, about that seasonality... we kind of know it's weekly, so we should be able to guess we're gonna see some signal at 7 lags. Nevertheless, let's check the acf: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib image size \n",
    "plt.rcParams['figure.figsize'] = (16, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(a_store, alpha=.05, lags=15)\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(a_store, alpha=.05, lags=15)\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... signal at 10 that does not seem exaplainable. Ah, the joys of Time Series! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Igor, fetch me the SARIMAX..._\n",
    "So extremely seasonal, as would be expected. Let's do some quick experiments with some hyper params, and try to fit a SARIMAX: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some dumb hyper params: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(a_store,             \n",
    "                          order=(0, 1, 0),              \n",
    "                          seasonal_order=(1, 1, 1, 1), \n",
    "                          enforce_stationarity=False,  \n",
    "                          enforce_invertibility=False) \n",
    "\n",
    "results = model.fit()\n",
    "results.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in the fact that we suspect there is some seasonality, and S should be 7: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(a_store,             \n",
    "                          order=(0, 1, 0),              \n",
    "                          seasonal_order=(1, 1, 1, 7), # <--- 7 \n",
    "                          enforce_stationarity=False,  \n",
    "                          enforce_invertibility=False) \n",
    "\n",
    "results = model.fit()\n",
    "results.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what should our `p` and `q` be, really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: \n",
    "> For p use the PACF  \n",
    "> For q use the ACF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(a_store,             \n",
    "                          order=(1, 1, 1),             # <---- 1, 1, 1    \n",
    "                          seasonal_order=(1, 1, 1, 7), # <--- 7 \n",
    "                          enforce_stationarity=False,  \n",
    "                          enforce_invertibility=False) \n",
    "\n",
    "results = model.fit()\n",
    "results.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe S should go up to 10? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(a_store,             \n",
    "                          order=(1, 1, 1),             # <---- 1, 1, 1    \n",
    "                          seasonal_order=(1, 1, 1, 10), # <--- 7 \n",
    "                          enforce_stationarity=False,  \n",
    "                          enforce_invertibility=False) \n",
    "\n",
    "results = model.fit()\n",
    "results.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once you've covered obvious options, try using hyper parameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nop, back to 7. Let's do a basic hyper parameter optimizer \n",
    "\n",
    "_Note: if your computer is pre-2015, you might want to use someone else's for this bit, it's kind of heavy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are feeling brave, increase the range to (0,3), \n",
    "# but it will take around 10 minutes on a good computer \n",
    "\n",
    "p = d = q = P = D = Q = range(0, 2)\n",
    "S = 7\n",
    "\n",
    "params_combinations = list(itertools.product(p, d, q, P, D, Q))\n",
    "\n",
    "inputs = [[x[0], x[1], x[2], x[3], x[4], x[5], S] for x in params_combinations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in one of the following functions (`get_best_params`), we will use a really nice package: `tqdm`. It allows you to show progress bars in python consoles and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aic(series_, params):\n",
    "    p = params[0] \n",
    "    d = params[1] \n",
    "    q = params[2] \n",
    "    P = params[3]\n",
    "    D = params[4] \n",
    "    Q = params[5]\n",
    "    S = params[6]\n",
    "    \n",
    "    model = sm.tsa.statespace.SARIMAX(series_,\n",
    "                                      order=(p, d, q),\n",
    "                                      seasonal_order=(P, D, Q, S),\n",
    "                                      enforce_stationarity=False,\n",
    "                                      enforce_invertibility=False)\n",
    "    results = model.fit()\n",
    "    \n",
    "    return results.aic\n",
    "\n",
    "\n",
    "def get_best_params(series_, inputs):\n",
    "    \n",
    "    aic_scores = {}\n",
    "    params_index = {}\n",
    "    for i in tqdm_notebook(range(len(inputs))):\n",
    "        try: \n",
    "            param_set = inputs[i]\n",
    "            aic = get_aic(series_, param_set) \n",
    "            aic_scores[i] = aic\n",
    "            params_index[i] = param_set\n",
    "\n",
    "        except Exception as e: \n",
    "            continue\n",
    "\n",
    "    temp = pd.DataFrame(params_index).T\n",
    "    temp.columns = ['p', 'd', 'q', 'P', 'D', 'Q', 'S']\n",
    "    temp['aic'] = pd.Series(aic_scores)\n",
    "    temp.sort_values('aic').head()\n",
    "\n",
    "    best_model_params = temp.aic.idxmin()\n",
    "\n",
    "    return temp.loc[best_model_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "best_params = get_best_params(a_store, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create a model with the winning params: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: _The results I put in the model aren't actually the ones that are in the HPO, because we ran an even bigger search and found these to be even better. However, unless you have time to go for a walk, don't try to go for range(3) on all params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(a_store,             \n",
    "                          order=(1, 0, 2),             \n",
    "                          seasonal_order=(1, 2, 2, 7),\n",
    "                          enforce_stationarity=False,  \n",
    "                          enforce_invertibility=False) \n",
    "\n",
    "results = model.fit()\n",
    "results.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the model in action: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(series_, pred_):\n",
    "    \n",
    "    \"\"\" \n",
    "    Remember Sam told us to build functions as we go? Let's not write this stuff again. \n",
    "    \"\"\"\n",
    "    \n",
    "    mean_predictions_ = pred_.predicted_mean\n",
    "\n",
    "    pred_ci_ = pred_.conf_int()\n",
    "    \n",
    "    series_.plot(label='observed')\n",
    "    mean_predictions_.plot(label='predicted', \n",
    "                           alpha=.7)\n",
    "\n",
    "    plt.fill_between(pred_ci_.index,\n",
    "                     pred_ci_.iloc[:, 0],\n",
    "                     pred_ci_.iloc[:, 1], \n",
    "                     color='k', \n",
    "                     alpha=.2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = results.get_prediction(dynamic=False)\n",
    "forecast = results.get_forecast(steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(a_store, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(a_store, forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... not particularly sure this beats our hand-crafted predictions. Still, let's try it out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting all stores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_sarimax(df_, store_nr, n_steps): \n",
    "    \n",
    "    store_ = df_.loc[idx[:, store_nr], 'Sales']\n",
    "    \n",
    "    store_.index = store_.index.droplevel('Store')\n",
    "\n",
    "    model = sm.tsa.statespace.SARIMAX(store_,             \n",
    "                              order=(1, 0, 1),             \n",
    "                              seasonal_order=(1, 1, 1, 7),\n",
    "                              enforce_stationarity=False,  \n",
    "                              enforce_invertibility=False) \n",
    "\n",
    "    results = model.fit()\n",
    "    return results.get_forecast(steps=n_steps).predicted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What stores do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = train.index.get_level_values('Store').unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING! If you have a slow computer, this might take a really, really long time. \n",
    "On a 2018 macbook pro, it takes about 60 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# This is another cell that will take a long time to run. \n",
    "\n",
    "res = {}\n",
    "\n",
    "for store_nr in tqdm_notebook(stores):\n",
    "    res[store_nr] = predict_with_sarimax(df_=new_train, store_nr=store_nr, n_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(res).unstack().reset_index()\n",
    "results.columns = ['Store', 'Date', 'Sales']\n",
    "results = results.set_index(['Date', 'Store']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do our results look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: using Joblib to speed up calculations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what computer you have, the previous cell can take a loooong time. When our code has cycles where each iteration does not need any information from previous iterations, we can use a **REALLY** cool package called [joblib](https://pythonhosted.org/joblib/), a package to make distributed computing really easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# This is another cell that will take a long time to run. \n",
    "\n",
    "def wrap_predict_with_sarimax(df_, store_nr): \n",
    "    return (store_nr, predict_with_sarimax(df_, store_nr, n_steps=4))\n",
    "\n",
    "res = Parallel(n_jobs=-1)(delayed(wrap_predict_with_sarimax)(df_=new_train, store_nr=store_nr) \n",
    "                          for store_nr in tqdm_notebook(stores))\n",
    "\n",
    "res = dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(res).unstack().reset_index()\n",
    "results.columns = ['Store', 'Date', 'Sales']\n",
    "results = results.set_index(['Date', 'Store']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results but \n",
    "\n",
    "**waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaay**\n",
    "\n",
    "faster :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how we did! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in results.index.get_level_values('Date').unique():\n",
    "    print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days_in_test_set:\n",
    "    new_test.loc[idx[day, :], 'predictions'] = results.loc[idx[day, :], 'Sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=new_test['Sales'], y_pred=new_test['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow... alright, that's kind of impressive. And we haven't added exogenous yet :)\n",
    "\n",
    "Also, we only tuned on one store, maybe we can do better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, Sam is screaming **_\"30 minutes to deliver!!\"_**. Panic fills the room. Let's predict the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep the test set \n",
    "_(in your heart of hearts, you know you should have made functions. But not Sam is screaming, so no time for that anymore)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Date = pd.to_datetime(test.Date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.set_index(['Date', 'Store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_index()  # <--- seriously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training data instead of the \"new train\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many days are we supposed to predict in the test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_test = test.index.get_level_values('Date').unique()\n",
    "sorted(days_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we need to forecast 4 days, from the full training set. We've pretty much done this before! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# This is another cell that will take a long time to run. \n",
    "\n",
    "res = {}\n",
    "\n",
    "for store_nr in tqdm_notebook(stores):\n",
    "    res[store_nr] = predict_with_sarimax(df_=train, store_nr=store_nr, n_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(res).unstack().reset_index()\n",
    "results.columns = ['Store', 'Date', 'Sales']\n",
    "results = results.set_index(['Date', 'Store']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days_in_test:\n",
    "    test.loc[idx[day, :], 'Sales'] = results.loc[idx[day, :], 'Sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivered! Time for some closing remarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You won't need this section to pass your BLU3 exercises, however it contains good advice for the Hackathon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your metric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose Mean Absolute Error, but depending on the problem, different metrics are preferable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a better metric? \n",
    "\n",
    "Imagine that we have the following targets and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'prediction': [970, 10, 15, 10000, 30, 90, 4700], \n",
    "    'target': [1000, 20, 5, 9000, 35, 70, 5001]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we use MAE, the error for the line 3 will be higher than the one we get for line 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abs. error'] = abs(df['prediction'] - df['target'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But does that mean that missing by 10 in line 2 is less important than missing by 1000 in line 3? Imagine that line 2 is the prediction and target for a really expensive product while line 3 is a cheap product. One way to weight the error would be to use the `sample_weight` parameter that sklearn offers in many of its metrics. But that would require us to decide what weights we would use for each error. An alternative to MAE that makes that evaluates the weight of each error is the [Mean Absolute Percentage Error (MAPE)](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abs. perc. error'] = abs((df['prediction'] - df['target']) / df['target'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with MAPE, the error for line 3 is not as important as the one for line 2. For MAPE, we relate the error unit to the desired target. The smaller the desired target is, the higher the impact of the error. We measured the errors for each line but we still need to compute the mean for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['abs. error', 'abs. perc. error']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, by the way, the dataset we used in this notebook was included in a [Kaggle challenge](https://www.kaggle.com/c/rossmann-store-sales) where the evaluation metric was [Root Mean Square Percentage Error](https://www.kaggle.com/c/rossmann-store-sales#evaluation). This metric is to RMSE as MAE is to MAPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing better train test splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there other ways of performing train-test split for time series? \n",
    "\n",
    "Unlike the datasets we used in the bootcamp and first hackathon where we wouldn't have any issues if we performed random train-test splits, in time series we must respect the order between train and test. Simply put, **all timestamps that appear in the train set must be smaller than the ones in test set**. A simple way to perform the train-test split in time series is to choose an arbitrary date like we did and split the time series into train and test. But, if we do this, we aren't able to check how well our model generalizes to different parts of the time series. An alternative train-test split method is the one implemented in sklearn, [TimeSeriesSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html). Before using it, let's extract the time series for store 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((train, test))\n",
    "all_data_for_store_1 = all_data.loc[idx[:, 1], :]\n",
    "all_data_for_store_1 = all_data_for_store_1.reset_index().drop('Store', axis=1).set_index('Date').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use `TimeSeriesSplit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(3, max_train_size=70)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for endo_train_idx, endo_test_idx in tss.split(all_data_for_store_1):\n",
    "    pd.Series(i, index=endo_train_idx).plot(label=\"Split {} (train)\".format(i))\n",
    "    pd.Series(i, index=endo_test_idx).plot(label=\"Split {} (test)\".format(i))\n",
    "    print(\"SPLIT {}\".format(i))\n",
    "    print('\\ttrain idx', '({} observations)'.format(len(endo_train_idx)), '\\n', endo_train_idx)\n",
    "    print('\\ttest idx', '({} observations)'.format(len(endo_test_idx)), '\\n', endo_test_idx)\n",
    "    i += 1\n",
    "    print('\\n')\n",
    "    \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, essentially, `TimeSeriesSplit` divides the dataset into *K* overlapping parts, where each one of those is divided into train and test. If you set `max_train_size` to an integer, the splitting algorithm will create splits where the train set has, at most, `max_train_size` observations. If you set `max_train_size=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(3, max_train_size=None)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for endo_train_idx, endo_test_idx in tss.split(all_data_for_store_1):\n",
    "    pd.Series(i, index=endo_train_idx).plot(label=\"Split {} (train)\".format(i))\n",
    "    pd.Series(i, index=endo_test_idx).plot(label=\"Split {} (test)\".format(i))\n",
    "    print(\"SPLIT {}\".format(i))\n",
    "    print('\\ttrain idx', '({} observations)'.format(len(endo_train_idx)), '\\n', endo_train_idx)\n",
    "    print('\\ttest idx', '({} observations)'.format(len(endo_test_idx)), '\\n', endo_test_idx)\n",
    "    i += 1\n",
    "    print('\\n')\n",
    "    \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the train set will always start at observation 0 but the its size will expand for each new split. Both of them can be used as a way to implement time series cross validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
